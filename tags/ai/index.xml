<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on The Tymscar Blog</title><link>/tags/ai/</link><description>Recent content in Ai on The Tymscar Blog</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 05 Aug 2025 20:20:00 +0000</lastBuildDate><atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Running OpenAI's GPT-OSS locally: the good, the bad, and the loopy</title><link>/posts/gptoss/</link><pubDate>Tue, 05 Aug 2025 20:20:00 +0000</pubDate><guid>/posts/gptoss/</guid><description>&lt;p>OpenAI recently launched something I didn&amp;rsquo;t expect to see from them: an open source model called &lt;a href="https://openai.com/index/introducing-gpt-oss/">GPT-OSS&lt;/a>. After years of keeping their models locked behind APIs, they&amp;rsquo;ve finally released both 20 billion and 120 billion parameter models that you can run entirely on your own hardware. Both can run on consumer grade hardware, but realistically most people would go with the 20 billion model since it fully fits into 16 gigs of VRAM on a GPU, which is why I went with that one as well.&lt;/p></description></item></channel></rss>