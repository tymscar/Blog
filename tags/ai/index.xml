<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on The Tymscar Blog</title><link>https://blog.tymscar.com/tags/ai/</link><description>Recent content in Ai on The Tymscar Blog</description><generator>Hugo</generator><language>en</language><lastBuildDate>Fri, 21 Nov 2025 12:00:00 +0000</lastBuildDate><atom:link href="https://blog.tymscar.com/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenAI Demo'd Fixing Issue #2472 Live. It's Still Open.</title><link>https://blog.tymscar.com/posts/openaiunmergeddemo/</link><pubDate>Fri, 21 Nov 2025 12:00:00 +0000</pubDate><guid>https://blog.tymscar.com/posts/openaiunmergeddemo/</guid><description>&lt;p&gt;During OpenAI&amp;rsquo;s GPT-5 launch event, they demoed the model&amp;rsquo;s ability to fix real bugs in production code. Live on stage. In their own repository. The kind of demo that makes CTOs reach for their credit cards and engineers nervously update their resumes. There&amp;rsquo;s just one small problem: the fix they promised to merge &amp;ldquo;right after the show&amp;rdquo; is still sitting there, unmerged, three and a half months later.&lt;/p&gt;
&lt;h2 id="the-demo"&gt;The Demo&lt;/h2&gt;
&lt;p&gt;At exactly 1 hour and 7 minutes into their launch video, they started working on issue &lt;a href="https://github.com/openai/openai-python/issues/2472"&gt;#2472&lt;/a&gt; in their openai-python repository.&lt;/p&gt;</description></item><item><title>Running OpenAI's GPT-OSS locally: the good, the bad, and the loopy</title><link>https://blog.tymscar.com/posts/gptoss/</link><pubDate>Tue, 05 Aug 2025 20:20:00 +0000</pubDate><guid>https://blog.tymscar.com/posts/gptoss/</guid><description>&lt;p&gt;OpenAI recently launched something I didn&amp;rsquo;t expect to see from them: an open source model called &lt;a href="https://openai.com/index/introducing-gpt-oss/"&gt;GPT-OSS&lt;/a&gt;. After years of keeping their models locked behind APIs, they&amp;rsquo;ve finally released both 20 billion and 120 billion parameter models that you can run entirely on your own hardware. Both can run on consumer grade hardware, but realistically most people would go with the 20 billion model since it fully fits into 16 gigs of VRAM on a GPU, which is why I went with that one as well.&lt;/p&gt;</description></item></channel></rss>